{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# unzip all files"
      ],
      "metadata": {
        "id": "KFicF7lss8bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def unzip_all(main_path, current_path=None):\n",
        "    \"\"\"\n",
        "    Recursively unzip all zip files starting from the main_path.\n",
        "    If current_path is None, it starts with main_path.\n",
        "    \"\"\"\n",
        "    if current_path is None:\n",
        "        current_path = main_path\n",
        "\n",
        "    # List all files and directories in the current path\n",
        "    for item in os.listdir(current_path):\n",
        "        item_path = os.path.join(current_path, item)\n",
        "\n",
        "        # If the item is a directory, recurse into it\n",
        "        if os.path.isdir(item_path):\n",
        "            unzip_all(main_path, item_path)\n",
        "        elif item.endswith('.zip'):\n",
        "            # Construct the directory path to extract the ZIP file into\n",
        "            extract_to = os.path.splitext(item_path)[0]\n",
        "\n",
        "            # Unzip the file into the directory\n",
        "            print(f\"Unzipping: {item_path}\")\n",
        "            shutil.unpack_archive(item_path, extract_to)\n",
        "\n",
        "            # Remove the ZIP file after extraction if desired\n",
        "            # os.remove(item_path)\n",
        "\n",
        "            # Recurse into the extracted directory in case there are nested ZIP files\n",
        "            unzip_all(main_path, extract_to)\n",
        "\n",
        "# Define the main path where your primary ZIP file is located\n",
        "MAIN_PATH = r\"C:\\Users\\shafi\\Downloads\\doi_10_5061_dryad_5hqbzkh6f__v20210917\"\n",
        "\n",
        "# Assuming the primary ZIP file is directly under MAIN_PATH and needs to be extracted first\n",
        "for item in os.listdir(MAIN_PATH):\n",
        "    if item.endswith('.zip'):\n",
        "        primary_zip_path = os.path.join(MAIN_PATH, item)\n",
        "        # Unzip the primary ZIP file\n",
        "        extract_to = os.path.splitext(primary_zip_path)[0]\n",
        "        print(f\"Unzipping primary ZIP: {primary_zip_path}\")\n",
        "        shutil.unpack_archive(primary_zip_path, extract_to)\n",
        "        # Now, unzip all other ZIP files recursively within the extracted directory\n",
        "        unzip_all(MAIN_PATH)\n",
        "\n",
        "print(\"Completed unzipping all files.\")\n"
      ],
      "metadata": {
        "id": "gXE3DnkOs73M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Combine each csv for each signal from files"
      ],
      "metadata": {
        "id": "KXRE6_HVtZJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Paths configuration\n",
        "DATA_PATH = r\"C:\\Users\\shafi\\Downloads\\doi_10_5061_dryad_5hqbzkh6f__v20210917\\Stress_dataset\\CE\"\n",
        "SAVE_PATH = r\"C:\\Users\\shafi\\Downloads\\doi_10_5061_dryad_5hqbzkh6f__v20210917\\Stress_dataset\\CE\\processed_data1\"\n",
        "\n",
        "# Create the save path directory if it doesn't exist\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Column names for the final combined CSV based on the signal type\n",
        "final_columns = {\n",
        "    'ACC': ['id', 'X', 'Y', 'Z', 'datetime'],\n",
        "    'EDA': ['id', 'EDA', 'datetime'],\n",
        "    'HR': ['id', 'HR', 'datetime'],\n",
        "    'TEMP': ['id', 'TEMP', 'datetime'],\n",
        "}\n",
        "\n",
        "# Initial empty dataframes for each signal\n",
        "signal_dataframes = {signal: pd.DataFrame(columns=cols) for signal, cols in final_columns.items()}\n",
        "\n",
        "# Function to process and return a dataframe from a CSV file, adding ID and datetime\n",
        "def process_signal_file(filepath, file_id, signal):\n",
        "    df = pd.read_csv(filepath, header=None)\n",
        "    start_timestamp, sample_rate = df.iloc[0, 0], df.iloc[1, 0]\n",
        "    df = df.iloc[2:]\n",
        "    df.columns = final_columns[signal][1:-1]  # Exclude 'id' and 'datetime'\n",
        "    df['id'] = file_id\n",
        "    df['datetime'] = pd.to_datetime(start_timestamp, unit='s') + pd.to_timedelta(df.index / sample_rate, unit='s')\n",
        "    return df\n",
        "\n",
        "# Walk through the dataset directory and process each signal CSV file\n",
        "for root, dirs, files in os.walk(DATA_PATH):\n",
        "    for directory in dirs:\n",
        "        dir_path = os.path.join(root, directory)\n",
        "        file_id = directory  # Assuming the directory name is the ID\n",
        "        for signal in final_columns.keys():\n",
        "            signal_file = f\"{signal}.csv\"\n",
        "            signal_filepath = os.path.join(dir_path, signal_file)\n",
        "            if os.path.isfile(signal_filepath):\n",
        "                df = process_signal_file(signal_filepath, file_id, signal)\n",
        "                signal_dataframes[signal] = pd.concat([signal_dataframes[signal], df])\n",
        "\n",
        "# Save the combined dataframes to CSV files\n",
        "for signal, df in signal_dataframes.items():\n",
        "    save_filepath = os.path.join(SAVE_PATH, f\"combined_{signal.lower()}.csv\")\n",
        "    df.to_csv(save_filepath, index=False)\n",
        "\n",
        "print('All CSV files have been combined and saved.')\n"
      ],
      "metadata": {
        "id": "Uaei4uXZtsLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9FEjy--F3Zc",
        "outputId": "38248ab4-d179-4841-b1d9-75742710b25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up ...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...Reading and preprocessing temp data...\n",
            "\n",
            "Merging data for ID: E4_1587318394Merging data for ID: E4_1587206108\n",
            "\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "\n",
            "Reading and preprocessing hr data...Reading and preprocessing temp data...Reading and preprocessing temp data...\n",
            "\n",
            "Merging data for ID: E4_1587324331\n",
            "Merging data for ID: E4_1587236732\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...Reading and preprocessing temp data...\n",
            "\n",
            "Merging data for ID: E4_1587241555\n",
            "Merging data for ID: E4_1587329010\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1587383217\n",
            "Merging data for ID: E4_1587294299\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...Reading and preprocessing hr data...\n",
            "\n",
            "Reading and preprocessing temp data...Reading and preprocessing temp data...\n",
            "\n",
            "Merging data for ID: E4_1587393366\n",
            "Merging data for ID: E4_1587298478\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1587302737Merging data for ID: E4_1587409032\n",
            "\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1587409707\n",
            "Merging data for ID: E4_1587312370\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...Reading and preprocessing hr data...\n",
            "\n",
            "Reading and preprocessing temp data...Reading and preprocessing temp data...\n",
            "\n",
            "Merging data for ID: E4_1587315090Merging data for ID: E4_1587417188\n",
            "\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1589312746\n",
            "Merging data for ID: E4_1588984019\n",
            "Reading and preprocessing acc data...Reading and preprocessing acc data...\n",
            "\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1589802835\n",
            "Merging data for ID: E4_1589028644\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1589811508\n",
            "Merging data for ID: E4_1589028974\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1589839893\n",
            "Merging data for ID: E4_1589051416\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1589889235\n",
            "Merging data for ID: E4_1589113150\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing eda data...\n",
            "Merging data for ID: E4_1589917617\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1589200218\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1590149482\n",
            "Merging data for ID: E4_1589306182\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1589306729\n",
            "Merging data for ID: E4_1592752787\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1592937530\n",
            "Merging data for ID: E4_1593104925\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1592937902\n",
            "Merging data for ID: E4_1593439701\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing hr data...\n",
            "Merging data for ID: E4_1592938003\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1593461673\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1592939478\n",
            "Merging data for ID: E4_1593538007\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1592945524\n",
            "Merging data for ID: E4_1593543088\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...Reading and preprocessing hr data...\n",
            "\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1593551794\n",
            "Merging data for ID: E4_1593002275\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1593612140\n",
            "Merging data for ID: E4_1593015418\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...Reading and preprocessing temp data...\n",
            "\n",
            "Merging data for ID: E4_1593624124\n",
            "Merging data for ID: E4_1593021695\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1593630180\n",
            "Merging data for ID: E4_1594057956\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1593711228\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1593726493\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1593786145\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1593809250\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1594043080\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1594049696\n",
            "Reading and preprocessing acc data...\n",
            "Reading and preprocessing eda data...\n",
            "Reading and preprocessing hr data...\n",
            "Reading and preprocessing temp data...\n",
            "Merging data for ID: E4_1594051892\n",
            "Saving merged data...\n"
          ]
        }
      ],
      "source": [
        "#code to combine all signal csv into one for each nurse\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import multiprocessing\n",
        "\n",
        "COMBINED_DATA_PATH = \"/content/drive/MyDrive/Stress_dataset/E4/processed_data1\"\n",
        "SAVE_PATH = \"/content/drive/MyDrive/Stress_dataset/E4/processed_data1/merged\"\n",
        "\n",
        "if not os.path.exists(SAVE_PATH):\n",
        "    os.makedirs(SAVE_PATH)\n",
        "\n",
        "print(\"Setting up ...\")\n",
        "\n",
        "signals = ['acc', 'eda', 'hr', 'temp']\n",
        "columns=['X', 'Y', 'Z', 'EDA', 'HR', 'TEMP', 'id', 'datetime']\n",
        "\n",
        "def optimize_dtype(df):\n",
        "    # This function can be enhanced based on actual data types in your CSVs\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = df[col].astype('float32')\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        df[col] = df[col].astype('int32')\n",
        "    return df\n",
        "\n",
        "def read_and_merge(signal):\n",
        "    print(f\"Reading and preprocessing {signal} data...\")\n",
        "    df = pd.read_csv(os.path.join(COMBINED_DATA_PATH, f\"combined_{signal}.csv\"), dtype={'id': str}, chunksize=10000)  # Adjust chunksize based on your dataset and memory capacity\n",
        "    optimized_dfs = []\n",
        "    for chunk in df:\n",
        "        optimized_chunk = optimize_dtype(chunk)\n",
        "        optimized_dfs.append(optimized_chunk)\n",
        "    return pd.concat(optimized_dfs, ignore_index=True)\n",
        "\n",
        "def merge_data(id, acc, eda, hr, temp):\n",
        "    print(f\"Merging data for ID: {id}\")\n",
        "    acc_id = acc[acc['id'] == id]\n",
        "    eda_id = eda[eda['id'] == id].drop(['id'], axis=1)\n",
        "    hr_id = hr[hr['id'] == id].drop(['id'], axis=1)\n",
        "    temp_id = temp[temp['id'] == id].drop(['id'], axis=1)\n",
        "\n",
        "    df = acc_id.merge(eda_id, on='datetime', how='outer')\n",
        "    df = df.merge(temp_id, on='datetime', how='outer')\n",
        "    df = df.merge(hr_id, on='datetime', how='outer')\n",
        "\n",
        "    df.fillna(method='ffill', inplace=True)\n",
        "    df.fillna(method='bfill', inplace=True)\n",
        "    return df\n",
        "\n",
        "def process_id(id):\n",
        "    acc = read_and_merge('acc')\n",
        "    eda = read_and_merge('eda')\n",
        "    hr = read_and_merge('hr')\n",
        "    temp = read_and_merge('temp')\n",
        "    merged_df = merge_data(id, acc, eda, hr, temp)\n",
        "    return merged_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example for reading a single file to get unique IDs, adjust based on your data structure\n",
        "    sample_df = pd.read_csv(os.path.join(COMBINED_DATA_PATH, \"combined_acc.csv\"), usecols=['id'], dtype={'id': str})\n",
        "    unique_ids = sample_df['id'].unique()\n",
        "\n",
        "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
        "    results = pool.map(process_id, unique_ids)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    final_df = pd.concat(results, ignore_index=True)\n",
        "    print(\"Saving merged data...\")\n",
        "    final_df.to_csv(os.path.join(SAVE_PATH, \"merged_data.csv\"), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Survey excel formating based on epoch timestamp\n",
        "# succesfull"
      ],
      "metadata": {
        "id": "XJ_HsN88HxkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Timestamp based trying to convert to timestamp\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load the Excel file\n",
        "excel_path = '/content/drive/MyDrive/Stress_dataset/SurveyResults.xlsx'  # Update this path\n",
        "df = pd.read_excel(excel_path)\n",
        "\n",
        "def process_row(row):\n",
        "    \"\"\"Generate secondly timestamps for each row and include additional columns.\"\"\"\n",
        "    # Ensure 'date' is a datetime object\n",
        "    date = pd.to_datetime(row['date'])\n",
        "\n",
        "    # Convert 'Start time' and 'End time' from Excel to timedelta\n",
        "    start_timedelta = timedelta(hours=row['Start time'].hour, minutes=row['Start time'].minute, seconds=row['Start time'].second)\n",
        "    end_timedelta = timedelta(hours=row['End time'].hour, minutes=row['End time'].minute, seconds=row['End time'].second)\n",
        "\n",
        "    # Combine 'date' with 'Start time' and 'End time' to create datetime objects\n",
        "    start_datetime = date + start_timedelta\n",
        "    end_datetime = date + end_timedelta\n",
        "\n",
        "    # Generate a range of secondly timestamps\n",
        "    secondly_timestamps = pd.date_range(start=start_datetime, end=end_datetime, freq='S')\n",
        "\n",
        "    # Convert secondly timestamps to epoch timestamps\n",
        "    epoch_timestamps = secondly_timestamps.astype('int64') // 10**9\n",
        "\n",
        "    # Create a DataFrame for each second in the range, including additional columns from the Excel file\n",
        "    data = {\n",
        "        'ID_Timestamp': [f\"{row['ID']}_{ts}\" for ts in epoch_timestamps],\n",
        "        'ID': row['ID'],\n",
        "        'Stress level': row['Stress level'],\n",
        "        'COVID related': row['COVID related'],\n",
        "        # Add other columns here as needed\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n",
        "processed_rows = [process_row(row) for _, row in df.iterrows()]\n",
        "\n",
        "# Combine all the processed rows into a single DataFrame\n",
        "expanded_df = pd.concat(processed_rows, ignore_index=True)\n",
        "\n",
        "# Export to CSV\n",
        "csv_output_path = '/content/drive/MyDrive/Stress_dataset/output_csv.csv'  # Update this path\n",
        "expanded_df.to_csv(csv_output_path, index=False)\n",
        "\n",
        "print(f'Data has been successfully processed and saved to {csv_output_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlmr_tDiMWIy",
        "outputId": "d9a8d1c5-8a1f-4105-8ff4-4c458f59ee71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been successfully processed and saved to /content/drive/MyDrive/Stress_dataset/output_csv.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#updated version for \"na\" value in survey excel file\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load the Excel file\n",
        "excel_path = '/content/drive/MyDrive/Stress_dataset/SurveyResults.xlsx'  # Update this path\n",
        "df = pd.read_excel(excel_path)\n",
        "\n",
        "def process_row(row):\n",
        "    \"\"\"Generate secondly timestamps for each row and include additional columns.\"\"\"\n",
        "    # Ensure 'date' is a datetime object\n",
        "    date = pd.to_datetime(row['date'])\n",
        "\n",
        "    # Convert 'Start time' and 'End time' from Excel to timedelta\n",
        "    start_timedelta = timedelta(hours=row['Start time'].hour, minutes=row['Start time'].minute, seconds=row['Start time'].second)\n",
        "    end_timedelta = timedelta(hours=row['End time'].hour, minutes=row['End time'].minute, seconds=row['End time'].second)\n",
        "\n",
        "    # Combine 'date' with 'Start time' and 'End time' to create datetime objects\n",
        "    start_datetime = date + start_timedelta\n",
        "    end_datetime = date + end_timedelta\n",
        "\n",
        "    # Generate a range of secondly timestamps\n",
        "    secondly_timestamps = pd.date_range(start=start_datetime, end=end_datetime, freq='S')\n",
        "\n",
        "    # Convert secondly timestamps to epoch timestamps\n",
        "    epoch_timestamps = secondly_timestamps.astype('int64') // 10**9\n",
        "\n",
        "    # Create a DataFrame for each second in the range, including additional columns from the Excel file\n",
        "    data = {\n",
        "        'ID_Timestamp': [f\"{row['ID']}_{ts}\" for ts in epoch_timestamps],\n",
        "        'ID': row['ID'],\n",
        "        'Stress level': row['Stress level'],\n",
        "        'COVID related': row['COVID related'],\n",
        "        # Add other columns here as needed\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n",
        "processed_rows = [process_row(row) for _, row in df.iterrows()]\n",
        "\n",
        "# Combine all the processed rows into a single DataFrame\n",
        "expanded_df = pd.concat(processed_rows, ignore_index=True)\n",
        "\n",
        "# Replace 'na' string values with 0 in the entire DataFrame\n",
        "expanded_df.replace('na', 0, inplace=True)\n",
        "\n",
        "# Replace NaN values with 0 in the entire DataFrame\n",
        "expanded_df.fillna(0, inplace=True)\n",
        "\n",
        "# Export to CSV\n",
        "csv_output_path = '/content/drive/MyDrive/Stress_dataset/output_csv_v2.csv'\n",
        "expanded_df.to_csv(csv_output_path, index=False)\n",
        "\n",
        "print(f'Data has been successfully processed and saved to {csv_output_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdL_ixYMgD3c",
        "outputId": "68f7b427-da4f-4042-ec6d-dbeb5106f65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been successfully processed and saved to /content/drive/MyDrive/Stress_dataset/output_csv_v2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dropping the extra column and final merge with nurses combined data based on Id_timestamp column"
      ],
      "metadata": {
        "id": "OC2bTNkzZwex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the processed data with 'ID_Timestamp' column\n",
        "processed_data_path = '/content/drive/MyDrive/Stress_dataset/output_csv.csv'  # Update with the actual path\n",
        "processed_data = pd.read_csv(processed_data_path)\n",
        "\n",
        "# Load the second dataset\n",
        "second_dataset_path = '/content/drive/MyDrive/Stress_dataset/15/processed_data1/merged/merged_data.csv'  # Update with the actual path\n",
        "second_dataset = pd.read_csv(second_dataset_path)\n",
        "\n",
        "# Merge the datasets on the 'ID_Timestamp' and 'ID' columns\n",
        "merged_dataset = pd.merge(processed_data, second_dataset, left_on='ID_Timestamp', right_on='id', how='inner')\n",
        "\n",
        "# Drop the 'ID_Timestamp' column from the first dataset and 'id' from the second dataset, if present\n",
        "columns_to_drop = ['ID_Timestamp', 'id']\n",
        "# Ensure only to drop columns that exist in the dataset to avoid KeyError\n",
        "columns_to_drop = [col for col in columns_to_drop if col in merged_dataset.columns]\n",
        "merged_dataset.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "# Save the merged and cleaned dataset\n",
        "merged_dataset_path = '/content/drive/MyDrive/Stress_dataset/15/processed_data1/Timestamp_merged_dataset_v2.csv'  # Update with the desired save path\n",
        "merged_dataset.to_csv(merged_dataset_path, index=False)\n",
        "\n",
        "print(f'Merged and cleaned dataset saved to: {merged_dataset_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wnni0T3POWO5",
        "outputId": "7c62e9ae-3dbd-447d-e138-799df0ec14d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged and cleaned dataset saved to: /content/drive/MyDrive/Stress_dataset/15/processed_data1/Timestamp_merged_dataset_v2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After merging everything Combining all csv into one"
      ],
      "metadata": {
        "id": "QsMDHKa4xZTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine all nurse timestamp combined data csv to one\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Directory where all CSV files are stored\n",
        "csv_directory = '/content/drive/MyDrive/Stress_dataset/all csv'  # Update with the actual directory path\n",
        "\n",
        "# List to hold data from each CSV file\n",
        "dataframes = []\n",
        "\n",
        "# Loop through the directory and read each CSV file\n",
        "for filename in os.listdir(csv_directory):\n",
        "    if filename.endswith('.csv'):  # Ensure to process only CSV files\n",
        "        file_path = os.path.join(csv_directory, filename)\n",
        "        # Read the CSV file and append it to the list of DataFrames\n",
        "        df = pd.read_csv(file_path)\n",
        "        dataframes.append(df)\n",
        "\n",
        "# Concatenate all DataFrames in the list into a single DataFrame\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Save the combined DataFrame to a new CSV file\n",
        "combined_csv_path = '/content/drive/MyDrive/Stress_dataset/all_combined_csv.csv'  # Update with the desired save path\n",
        "combined_df.to_csv(combined_csv_path, index=False)\n",
        "\n",
        "print(f'All CSV files have been combined and saved to {combined_csv_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y2mAfXXQ_cV",
        "outputId": "e1d0dc6f-b935-455d-8912-9b652ffa8798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-198e051773f7>:17: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "<ipython-input-36-198e051773f7>:17: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "<ipython-input-36-198e051773f7>:17: DtypeWarning: Columns (1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All CSV files have been combined and saved to /content/drive/MyDrive/Stress_dataset/all_combined_csv.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/Stress_dataset/all_combined_csv.csv')\n",
        "display(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "nn1HbaVEuswn",
        "outputId": "1280017f-d997-4530-e090-f0ad1854960a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-5583a140cb8d>:1: DtypeWarning: Columns (0,1,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train = pd.read_csv('/content/drive/MyDrive/Stress_dataset/all_combined_csv.csv')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          ID Stress level COVID related     X     Y     Z  \\\n",
              "0         7E            0             0 -37.0  10.0 -51.0   \n",
              "1         7E            0             0 -36.0   8.0 -53.0   \n",
              "2         7E            0             0 -36.0   8.0 -55.0   \n",
              "3         7E            0             0 -37.0  10.0 -55.0   \n",
              "4         7E            0             0 -31.0   4.0 -55.0   \n",
              "...       ..          ...           ...   ...   ...   ...   \n",
              "10008089  15            2             0   9.0 -78.0  15.0   \n",
              "10008090  15            2             0   9.0 -78.0  15.0   \n",
              "10008091  15            2             0   9.0 -78.0  15.0   \n",
              "10008092  15            2             0   9.0 -78.0  15.0   \n",
              "10008093  15            2             0   9.0 -78.0  15.0   \n",
              "\n",
              "                            datetime       EDA   TEMP     HR  \n",
              "0         2020-11-05 14:43:48.062500  0.000000  33.09  98.00  \n",
              "1         2020-11-05 14:43:48.093750  0.000000  33.09  98.00  \n",
              "2         2020-11-05 14:43:48.125000  0.000000  33.09  98.00  \n",
              "3         2020-11-05 14:43:48.156250  0.000000  33.09  98.00  \n",
              "4         2020-11-05 14:43:48.187500  0.000000  33.09  98.00  \n",
              "...                              ...       ...    ...    ...  \n",
              "10008089         2020-07-24 15:45:11  0.207538  33.15  93.63  \n",
              "10008090         2020-07-24 15:45:12  0.207538  33.15  93.60  \n",
              "10008091         2020-07-24 15:45:13  0.207538  33.15  93.55  \n",
              "10008092         2020-07-24 15:45:14  0.207538  33.15  93.47  \n",
              "10008093         2020-07-24 15:45:15  0.207538  33.15  93.38  \n",
              "\n",
              "[10008094 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1531bc67-28ff-46c2-929f-6600b2693832\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Stress level</th>\n",
              "      <th>COVID related</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "      <th>datetime</th>\n",
              "      <th>EDA</th>\n",
              "      <th>TEMP</th>\n",
              "      <th>HR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7E</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-37.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-51.0</td>\n",
              "      <td>2020-11-05 14:43:48.062500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.09</td>\n",
              "      <td>98.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7E</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-36.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>-53.0</td>\n",
              "      <td>2020-11-05 14:43:48.093750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.09</td>\n",
              "      <td>98.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7E</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-36.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>-55.0</td>\n",
              "      <td>2020-11-05 14:43:48.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.09</td>\n",
              "      <td>98.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7E</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-37.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-55.0</td>\n",
              "      <td>2020-11-05 14:43:48.156250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.09</td>\n",
              "      <td>98.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7E</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-31.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-55.0</td>\n",
              "      <td>2020-11-05 14:43:48.187500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>33.09</td>\n",
              "      <td>98.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10008089</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-78.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>2020-07-24 15:45:11</td>\n",
              "      <td>0.207538</td>\n",
              "      <td>33.15</td>\n",
              "      <td>93.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10008090</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-78.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>2020-07-24 15:45:12</td>\n",
              "      <td>0.207538</td>\n",
              "      <td>33.15</td>\n",
              "      <td>93.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10008091</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-78.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>2020-07-24 15:45:13</td>\n",
              "      <td>0.207538</td>\n",
              "      <td>33.15</td>\n",
              "      <td>93.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10008092</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-78.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>2020-07-24 15:45:14</td>\n",
              "      <td>0.207538</td>\n",
              "      <td>33.15</td>\n",
              "      <td>93.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10008093</th>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-78.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>2020-07-24 15:45:15</td>\n",
              "      <td>0.207538</td>\n",
              "      <td>33.15</td>\n",
              "      <td>93.38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10008094 rows × 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1531bc67-28ff-46c2-929f-6600b2693832')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1531bc67-28ff-46c2-929f-6600b2693832 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1531bc67-28ff-46c2-929f-6600b2693832');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-46b75711-7f0f-4e66-b565-492afb18862d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-46b75711-7f0f-4e66-b565-492afb18862d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-46b75711-7f0f-4e66-b565-492afb18862d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First attempt with adding Secondly timestamp in survey data  \n",
        "# Not succesfull"
      ],
      "metadata": {
        "id": "ChJltGSTvkVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#converting survey excel to secondly timestamp\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "excel_path = '/content/drive/MyDrive/Stress_dataset/SurveyResults.xlsx'  # Update this path\n",
        "df = pd.read_excel(excel_path)\n",
        "\n",
        "def process_row(row):\n",
        "    \"\"\"Generate secondly timestamps for each row and include additional columns.\"\"\"\n",
        "    # Combine 'date' with 'Start time' and 'End time' to create datetime objects\n",
        "    start_datetime = pd.to_datetime(row['date']) + timedelta(hours=row['Start time'].hour, minutes=row['Start time'].minute, seconds=row['Start time'].second)\n",
        "    end_datetime = pd.to_datetime(row['date']) + timedelta(hours=row['End time'].hour, minutes=row['End time'].minute, seconds=row['End time'].second)\n",
        "\n",
        "    # Generate a range of secondly timestamps\n",
        "    secondly_timestamps = pd.date_range(start=start_datetime, end=end_datetime, freq='S')\n",
        "\n",
        "    # Create a DataFrame for each second in the range, including additional columns from the Excel file\n",
        "    data = {\n",
        "        'Secondly Timestamps': secondly_timestamps,\n",
        "        'ID': row['ID'],\n",
        "        # Add other columns here\n",
        "        'Stress level': row['Stress level'],\n",
        "        'COVID related': row['COVID related'],\n",
        "        # You can add as many additional columns as needed\n",
        "    }\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n",
        "processed_rows = [process_row(row) for _, row in df.iterrows()]\n",
        "\n",
        "# Combine all the processed rows into a single DataFrame\n",
        "expanded_df = pd.concat(processed_rows, ignore_index=True)\n",
        "\n",
        "# Export to CSV\n",
        "csv_output_path = '/content/drive/MyDrive/Stress_dataset/output_csv.csv'  # Update this path\n",
        "expanded_df.to_csv(csv_output_path, index=False)\n",
        "\n",
        "print(f'Data has been successfully processed and saved to {csv_output_path}')\n"
      ],
      "metadata": {
        "id": "6Mj-4CAV8Vs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting each nurse data from survey excel file\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/Stress_dataset/survey_csv.csv'  # Adjust the file path as necessary\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Identify all unique nurse IDs\n",
        "unique_ids = data['ID'].unique()\n",
        "\n",
        "# Directory to save individual nurse data; ensure this directory exists or is created\n",
        "output_directory = '/content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_data'\n",
        "\n",
        "# Iterate through each unique ID to filter and save data\n",
        "for nurse_id in unique_ids:\n",
        "    # Filter data for the current nurse ID\n",
        "    nurse_data = data[data['ID'] == nurse_id]\n",
        "\n",
        "    # Save the filtered data to a CSV file named after the nurse ID\n",
        "    nurse_data_file_path = f'{output_directory}{nurse_id}_data.csv'\n",
        "    nurse_data.to_csv(nurse_data_file_path, index=False)\n",
        "    print(f'Data for Nurse ID {nurse_id} saved to {nurse_data_file_path}')\n",
        "\n",
        "# If you need to merge these individual files later into a combined dataset\n",
        "# you can read each file and concatenate them into a single DataFrame\n",
        "combined_data_frames = []\n",
        "for nurse_id in unique_ids:\n",
        "    nurse_data_file_path = f'{output_directory}{nurse_id}_data.csv'\n",
        "    nurse_data = pd.read_csv(nurse_data_file_path)\n",
        "    combined_data_frames.append(nurse_data)\n",
        "\n",
        "# Combine all the DataFrames into one\n",
        "combined_data = pd.concat(combined_data_frames, ignore_index=True)\n",
        "\n",
        "# Optionally, save the combined data to a new CSV file\n",
        "combined_data_file_path = '/content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/combined_nurse_data.csv'\n",
        "combined_data.to_csv(combined_data_file_path, index=False)\n",
        "print(f'Combined data saved to {combined_data_file_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9RAN-giIQTz",
        "outputId": "a0200789-d1c9-46e2-a693-b08af989784b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for Nurse ID 5C saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_data5C_data.csv\n",
            "Data for Nurse ID E4 saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_dataE4_data.csv\n",
            "Data for Nurse ID 7A saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_data7A_data.csv\n",
            "Data for Nurse ID 94 saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_data94_data.csv\n",
            "Data for Nurse ID CE saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_dataCE_data.csv\n",
            "Data for Nurse ID 6B saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_data6B_data.csv\n",
            "Data for Nurse ID 6D saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_data6D_data.csv\n",
            "Data for Nurse ID 15 saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_data15_data.csv\n",
            "Data for Nurse ID F5 saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_dataF5_data.csv\n",
            "Data for Nurse ID DF saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_dataDF_data.csv\n",
            "Data for Nurse ID 8B saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_data8B_data.csv\n",
            "Data for Nurse ID 7E saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_data7E_data.csv\n",
            "Data for Nurse ID 83 saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_data83_data.csv\n",
            "Data for Nurse ID BG saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_dataBG_data.csv\n",
            "Data for Nurse ID EG saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_dataEG_data.csv\n",
            "Combined data saved to /content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/combined_nurse_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# merging sensor data with survey data by secondly timestamp"
      ],
      "metadata": {
        "id": "n_G6Ja5Z7yDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sorting both dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define file paths for the input datasets\n",
        "nurse_data_path = '/content/drive/MyDrive/Stress_dataset/Survey csv for each nurse/nurse_dataEG_data.csv'  # Update this path as needed\n",
        "second_dataset_path = '/content/drive/MyDrive/Stress_dataset/EG/processed_data1/merged/merged_data.csv'  # Update this path as needed\n",
        "\n",
        "# Define file paths for the output sorted datasets\n",
        "output_nurse_data_sorted_path = '/content/drive/MyDrive/Stress_dataset/EG/nurse_dataEG_data_sorted.csv'\n",
        "output_second_dataset_sorted_path = '/content/drive/MyDrive/Stress_dataset/EG/second_dataset_sorted.csv'\n",
        "\n",
        "# Load the nurse dataset and sort it by the 'Secondly Timestamps' column\n",
        "nurse_data = pd.read_csv(nurse_data_path, parse_dates=['Secondly Timestamps'])\n",
        "nurse_data_sorted = nurse_data.sort_values(by='Secondly Timestamps')\n",
        "# Save the sorted nurse dataset\n",
        "nurse_data_sorted.to_csv(output_nurse_data_sorted_path, index=False)\n",
        "print(f\"Nurse dataset sorted and saved to: {output_nurse_data_sorted_path}\")\n",
        "\n",
        "# Load the second dataset and sort it by the 'datetime' column\n",
        "second_dataset = pd.read_csv(second_dataset_path, parse_dates=['datetime'])\n",
        "second_dataset_sorted = second_dataset.sort_values(by='datetime')\n",
        "# Save the sorted second dataset\n",
        "second_dataset_sorted.to_csv(output_second_dataset_sorted_path, index=False)\n",
        "print(f\"Second dataset sorted and saved to: {output_second_dataset_sorted_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oR_qR7N1-Jk",
        "outputId": "b55ab0f0-9292-4148-9f33-6fe8f2d2f47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nurse dataset sorted and saved to: /content/drive/MyDrive/Stress_dataset/EG/nurse_dataEG_data_sorted.csv\n",
            "Second dataset sorted and saved to: /content/drive/MyDrive/Stress_dataset/EG/second_dataset_sorted.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#merging both dataset after sorting\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Paths to the datasets\n",
        "nurse_data_path = '/content/drive/MyDrive/Stress_dataset/E4/nurse_dataE4_data_sorted.csv'  # Update with the actual path\n",
        "second_dataset_path = '/content/drive/MyDrive/Stress_dataset/E4/second_dataset_sorted.csv'  # Update with the sorted second dataset path\n",
        "\n",
        "# Load the nurse dataset\n",
        "nurse_data = pd.read_csv(nurse_data_path, parse_dates=['Secondly Timestamps'])\n",
        "\n",
        "# Load the second dataset\n",
        "second_dataset = pd.read_csv(second_dataset_path, parse_dates=['datetime'])\n",
        "\n",
        "# Ensure both datasets are sorted by their datetime columns\n",
        "nurse_data_sorted = nurse_data.sort_values(by='Secondly Timestamps')\n",
        "second_dataset_sorted = second_dataset.sort_values(by='datetime')\n",
        "\n",
        "# Perform an exact match merge on the datetime columns\n",
        "merged_dataset = pd.merge(nurse_data_sorted, second_dataset_sorted, left_on='Secondly Timestamps', right_on='datetime', how='inner')\n",
        "\n",
        "# Save the merged dataset\n",
        "output_merged_dataset_path = '/content/drive/MyDrive/Stress_dataset/E4/merged_nurse_and_second_dataset.csv'  # Adjust the filename as needed\n",
        "merged_dataset.to_csv(output_merged_dataset_path, index=False)\n",
        "\n",
        "print(f\"Merged dataset saved to: {output_merged_dataset_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuQXMBrvJYu3",
        "outputId": "22c6acc4-22f7-4b41-a111-68111d1e2532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged dataset saved to: /content/drive/MyDrive/Stress_dataset/E4/merged_nurse_and_second_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# validation"
      ],
      "metadata": {
        "id": "ZE_FkBR-wf10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking datetime in each dataset if its exist\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/Stress_dataset/5C/processed_data1/Timestamp_merged_dataset.csv'  # Adjust this to the path of your CSV file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Specify the column name that contains date values and the date you are searching for\n",
        "date_column = 'datetime'  # Replace 'YourDateColumnName' with the actual column name\n",
        "search_date = '15/04/2020'  # Adjust this to the date you are looking for, formatted as a string\n",
        "\n",
        "# Convert the date column to datetime format (if not already in datetime format)\n",
        "data[date_column] = pd.to_datetime(data[date_column])\n",
        "\n",
        "# Convert the search date to datetime format for accurate comparison\n",
        "search_date = pd.to_datetime(search_date)\n",
        "\n",
        "# Check if the date exists in the column\n",
        "date_exists = search_date in data[date_column].values\n",
        "\n",
        "# Print the result\n",
        "if date_exists:\n",
        "    print(f\"The date {search_date.date()} exists in the dataset.\")\n",
        "else:\n",
        "    print(f\"The date {search_date.date()} does not exist in the dataset.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "l7Db_f6NV6oD",
        "outputId": "0bc4ad90-195a-4c46-b63d-a72b5f44edc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The date 2020-04-15 exists in the dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-1816849f43ca>:17: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
            "  search_date = pd.to_datetime(search_date)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1816849f43ca>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate_exists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The date {search_date.date()} exists in the dataset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The date {search_date.date()} does not exist in the dataset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ce4eypp-QMQ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}